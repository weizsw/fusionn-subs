# Design: Switch to OpenRouter

## Context

The current implementation is tightly coupled to Gemini's API through the `gemini-subtrans.sh` script from the llm-subtrans project. The llm-subtrans project supports multiple providers including OpenRouter, which acts as a unified gateway to multiple LLM providers.

OpenRouter advantages:

- Access to 100+ models from various providers (OpenAI, Anthropic, Google, Meta, etc.)
- Unified API and billing
- Better rate limits and pricing flexibility
- Can still use Gemini models through OpenRouter (e.g., `google/gemini-2.0-flash-exp`)

## Goals / Non-Goals

### Goals

- Replace Gemini-specific configuration with OpenRouter configuration
- Maintain backward compatibility in config hot-reload behavior
- Keep the same translator interface and architecture
- Use llm-subtrans's built-in OpenRouter support (no custom API integration)

### Non-Goals

- Supporting multiple providers simultaneously (single provider at a time)
- Direct API integration (continue using llm-subtrans scripts)
- Changing the worker/callback architecture
- Adding provider-specific features beyond basic translation

## Decisions

### Decision 1: Use llm-subtrans OpenRouter Script

**What**: Invoke llm-subtrans with `--provider openrouter` flag instead of using `gemini-subtrans.sh`

**Why**:

- llm-subtrans already has OpenRouter support built-in
- Maintains consistency with upstream project
- No need to implement custom API client
- Easier to get updates and bug fixes

**Alternatives considered**:

- Direct OpenRouter API integration: More complex, duplicates llm-subtrans work
- Keep Gemini and add OpenRouter: Unnecessary complexity for single-provider use case

### Decision 2: Use llm-subtrans.sh Script (OpenRouter Default)

**What**: Use `llm-subtrans.sh` script generated by installation, which defaults to OpenRouter

**Why**:

- `llm-subtrans.sh` is generated by `install.sh` and defaults to OpenRouter
- More flexible for future provider changes
- Cleaner configuration model
- No additional provider installation needed (OpenRouter is default)
- Aligns with llm-subtrans's recommended usage pattern

**Installation approach** (for Dockerfile):

```bash
cd /opt/llm-subtrans
# Run non-interactive installation
printf "2\n\n0\n" | ./install.sh
# 2 = CLI tools only (no GUI)
# (blank) = Skip OpenRouter API key configuration (passed at runtime)
# 0 = No additional providers (OpenRouter is default)
# This generates llm-subtrans.sh in the installation directory
```

**Current Dockerfile** (installs Gemini provider):

```dockerfile
RUN set -e; printf "2\n\n2\n\n" | ./install.sh
# 2 = CLI tools
# (blank) = Skip API key
# 2 = Google Gemini provider
# (blank) = Complete installation
# → Generates gemini-subtrans.sh
```

**Updated Dockerfile** (uses OpenRouter as default):

```dockerfile
RUN set -e; printf "2\n\n0\n" | ./install.sh
# 2 = CLI tools only
# (blank) = Skip API key
# 0 = No additional providers (OpenRouter is default)
# → Generates llm-subtrans.sh
```

**Command structure**:

```bash
# Old (Gemini-specific)
gemini-subtrans.sh input.srt -k API_KEY -m MODEL --target_language Chinese

# New (llm-subtrans.sh defaults to OpenRouter)
/opt/llm-subtrans/llm-subtrans.sh input.srt \
  --model google/gemini-2.0-flash-exp \
  -l Chinese \
  --apikey API_KEY \
  --ratelimit 10 \
  --maxbatchsize 20 \
  --instruction "Custom instructions..."
```

**Note**: The model format uses `provider/model` convention (e.g., `google/gemini-2.0-flash-exp`, `openai/gpt-4o-mini`, `anthropic/claude-3-5-sonnet`)

### Decision 3: Config Schema Changes

**What**: Replace `gemini` section with `openrouter` section

**Proposed structure**:

```yaml
openrouter:
  api_key: ""                    # OpenRouter API key (required)
  model: "openai/gpt-4o-mini"    # Model in provider/model format (required)
  instruction: ""                # Custom translation instruction (optional)
  max_batch_size: 20             # Batch size for translation (optional)
  rate_limit: 10                 # Requests per minute (optional)
```

**Why**:

- Clear separation from Gemini
- OpenRouter has fixed endpoint (no base_url needed)
- Matches llm-subtrans parameter names
- Simple, focused on current use case
- Can be made more abstract later if we need to support multiple providers

### Decision 4: Environment Variable Naming

**What**: Use `FUSIONN_SUBS_OPENROUTER_*` prefix for config, `LLM_SUBTRANS_SCRIPT_PATH` for script location

**Why**:

- Consistent with current naming pattern
- Clear indication of provider
- Easy to migrate existing configs
- `LLM_SUBTRANS_SCRIPT_PATH` separates infrastructure from business config

**Environment variables**:

- `FUSIONN_SUBS_OPENROUTER_API_KEY` - OpenRouter API key
- `FUSIONN_SUBS_OPENROUTER_MODEL` - Model in provider/model format
- `LLM_SUBTRANS_SCRIPT_PATH` - Path to llm-subtrans.sh (default: `/opt/llm-subtrans/llm-subtrans.sh`)
- `LLM_SUBTRANS_DIR` - Installation directory (default: `/opt/llm-subtrans`)

## Risks / Trade-offs

### Risk 1: Breaking Change for Existing Users

**Impact**: All existing deployments must update config files

**Mitigation**:

- Provide clear migration guide in README
- Document equivalent OpenRouter models for Gemini users
- Consider adding config validation error messages that guide migration

### Risk 2: llm-subtrans Script Interface Changes

**Impact**: Upstream changes to llm-subtrans could break invocation

**Mitigation**:

- Pin llm-subtrans version in Dockerfile (use git tag instead of `--depth 1`)
- Document tested llm-subtrans version
- Add integration tests for script invocation

### Risk 3: Different Error Patterns

**Impact**: OpenRouter may return different errors than Gemini

**Mitigation**:

- Review llm-subtrans error handling for OpenRouter
- Update `detectScriptFailure()` if needed
- Test with various error scenarios

## Migration Plan

### For Users

1. Obtain OpenRouter API key from <https://openrouter.ai/>
2. Update `config.yaml`:
   - Rename `gemini` section to `openrouter`
   - Update `api_key` with OpenRouter key
   - Update `model` to OpenRouter format (e.g., `google/gemini-2.0-flash-exp` for Gemini)
3. Update environment variables if using env-based config
4. Restart service

### For Deployment

1. Pull new Docker image
2. Update config volume mount with new config format
3. Update environment variables in docker-compose or orchestration config
4. Restart containers

### Rollback

If issues arise, users can:

- Revert to previous Docker image tag
- Restore old config file
- No data migration needed (stateless service)

## Decisions (Answered)

### Keep Gemini Implementation

**Decision**: Keep the old `GeminiTranslator` code as a fallback option.

**Why**:

- Provides rollback capability if OpenRouter has issues
- Users who prefer direct Gemini API can still use it
- Minimal maintenance cost to keep existing working code

**Implementation**: Keep both implementations, select based on config section present.

### Abstract Translator Pattern

**Decision**: Create abstract translator with provider selection for future flexibility.

**Why**:

- Easy to add new providers (Anthropic, local models, etc.)
- Clean separation of concerns
- Provider-specific logic isolated
- Config-driven selection

**Pattern**:

```go
type Translator interface {
    Translate(ctx context.Context, msg types.JobMessage) (string, error)
}

type TranslatorFactory struct {}

func (f *TranslatorFactory) Create(cfg *config.Config) (Translator, error) {
    // Check which config section exists
    if cfg.OpenRouter.APIKey != "" {
        return NewOpenRouterTranslator(cfg.OpenRouter), nil
    }
    if cfg.Gemini.APIKey != "" {
        return NewGeminiTranslator(cfg.Gemini), nil
    }
    return nil, errors.New("no valid translator config found")
}
```

### Universal Rate Limiting

**Decision**: Use a conservative rate limit that works across all providers.

**Why**:

- Different providers have different rate limits
- Users can tune based on their specific plan
- Better to start conservative and increase than hit limits

**Approach**:

- Default: 10 requests/minute (safe for most providers)
- Configurable per provider section
- Document recommended limits per provider in README
